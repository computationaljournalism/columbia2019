{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to Web Scraping\n",
    "\n",
    "We've been collecting data using simple interfaces -- extensions to Chrome. As we mentioned, while open data portals, APIs and other publication mechanisms provide easy ways to get to information we need for our analysys and reporting, there are plenty other valuable data sources for us to take advantage of: web pages (HTML), PDF files, email dumps, etc. Automating the extraction of useful information from web pages is known as **\"web scraping.\"** A terrible name aside, web scraping is very powerful and it's something you'll want to master. Today, we'll close our session talking about some of the basics of web scraping in Python.\n",
    "\n",
    "![Web Scraping](https://blog.hartleybrody.com/wp-content/uploads/2012/12/scraper-tool.jpg)\n",
    "\n",
    "\n",
    "There are many ways to scrape information from the web, but we're going to use Python, [requests](http://docs.python-requests.org/en/master/) and [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trump's Lies\n",
    "\n",
    "Since we haven't talked about Trump nearly enough in this class(!), let's take a look at a New York Times piece on [all of the lies Trump told in 2017](https://www.nytimes.com/interactive/2017/06/23/opinion/trumps-lies.html). Fun! This will lead to a good scraping exercise. Before we get to the code, take a quick look at the NYTimes piece.\n",
    "\n",
    "The first part of web scraping is making HTTP requests to pull the pages we need. We will use the [requests](http://docs.python-requests.org/en/master/) library?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the request to get the Trump Lies HTML\n",
    "from requests import get\n",
    "\n",
    "url = 'https://www.nytimes.com/interactive/2017/06/23/opinion/trumps-lies.html'\n",
    "\n",
    "headers = {\n",
    "    'From': '<put your email here>',\n",
    "}\n",
    "response = get(url, headers=headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we include a \"header\" field (represented as a dictionary). The header passes information to the web server that might change the way it returns content. In later exercises, we might need to specify the header \"User-Agent\" which tells the server what kind of  browser the requeste is being made from -- some servers don't like handing pages out to bots. \n",
    "\n",
    "For now, we are using the \"From\" header to announce ourselves. I like to tell a source that I'm taking data. If you want to know more about headers, have a look [here](https://en.wikipedia.org/wiki/List_of_HTTP_header_fields)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see what we have. remember that response.text will give us a string value of the page HTML\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is kind of a mess. The whole web page has been read in as a string. Thankfully, one of the great things about Python is a package called [BeautifulSoup](http://www.crummy.com/software/BeautifulSoup/), designed by [Leonard Richardson](http://www.crummy.com/self/). It is truly a thing of beauty. BeautifulSoup is a parser for HTML (and XML) that creates an object that lets you interact with the components of a web page. You can search for tags, extract attributes from the tags and pull the content contained in a tag. [The documentation is pretty simple too.](http://www.crummy.com/software/BeautifulSoup/bs4/doc/) The latest version of BeautifulSoup is 4.6.0 and the package is called bs4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will come back to parsing Trump's lies with BeautifulSoup but let's start with a simple example first. Here is some very simple HTML that I'd like to run through BeautifulSoup:\n",
    "\n",
    "```html\n",
    "<html>\n",
    "\n",
    "    <head>\n",
    "        <title>My Technology News Site</title>\n",
    "    </head>\n",
    "\n",
    "    <body>\n",
    "        <div>\n",
    "            <p class=\"title\"><strong>Steve Jobs introduces the public beta of Mac OS X</strong></p>\n",
    "            <div class=\"description\">Sept 13, 2000 - Steve Jobs <a href=\"https://www.apple.com/pr/library/2000/09/13Apple-Releases-Mac-OS-X-Public-Beta.html\" target=\"_blank\">introduces</a> the public beta of Mac OS X for US$29.95.</div>\n",
    "            <div class=\"author\">Author: Michael Young</div>\n",
    "        </div>\n",
    "    </body>\n",
    "\n",
    "</html>\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "our_html = '''\n",
    "<html>\n",
    "\n",
    "    <head>\n",
    "        <title>My Technology News Site\n",
    "    </head>\n",
    "\n",
    "    <body>\n",
    "        <div>\n",
    "            <p class=\"title\"><strong>Steve Jobs introduces the public beta of Mac OS X</strong></p>\n",
    "            <div class=\"description\">Sept 13, 2000 - Steve Jobs <a href=\"https://www.apple.com/pr/library/2000/09/13Apple-Releases-Mac-OS-X-Public-Beta.html\" target=\"_blank\">introduces</a> the public beta of Mac OS X for US$29.95.</div>\n",
    "            <div class=\"author\">Author: Michael Young</div>\n",
    "        </div>\n",
    "    </body>\n",
    "\n",
    "</html>\n",
    "'''\n",
    "\n",
    "# BeautifulSoup takes two arguments: a string (hopefully with HTML in it) and the parser we'd like to use\n",
    "soup = BeautifulSoup(our_html, 'html.parser')\n",
    "\n",
    "# print out a pretty version of the BeautifulSoup object\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a super-quick review of [HTML](https://en.wikipedia.org/wiki/HTML):\n",
    "\n",
    "Hypertext Markup Language is the language used for creating web pages. HTML uses `tags` which help label as well as structure the data in the document. Web browsers use the tags to help render the web page but does not display the tags. \n",
    "\n",
    "`Tags` normall come in pairs and have an opening tag `<p>` and a closing tag `</p>`:\n",
    "```html\n",
    "<p>this is a paragraph tag</p>\n",
    "```\n",
    "\n",
    "Other tags like the image tag `<img>` don't have a closing tag:\n",
    "```html\n",
    "<img src=\"http://somesite.com/images/logo.jpg\" />\n",
    "```\n",
    "\n",
    "The other important thing about HTML tags is that they can contain one or more `attributes`. Like in the `<img>` tag above, the `src` attribute is used specify the URL of the image. A tag with multiple attributes could look like this:\n",
    "```html\n",
    "<p attribute_1=\"value1\" attribute_2=\"value2\">\n",
    "Our content goes here\n",
    "</p>\n",
    "```\n",
    "\n",
    "HTML documents typically have nested tags (think of a tree!) that looks like this:\n",
    "```html\n",
    "<html>\n",
    "  <head>\n",
    "    <title>My First Website!</title>\n",
    "  </head>\n",
    "\n",
    "    <body>\n",
    "        <p>My mom would be proud of this.</p>\n",
    "    </body>  \n",
    "</html>\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back to BeautifulSoup...\n",
    "\n",
    "When we run our HTML document through BeautifulSoup, we get a python object that allows us to traverse, query and manipulate the HTML document.\n",
    "\n",
    "Here are a few ways to inspect our simple HTML document that we loaded above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <title> tag\n",
    "print(soup.title)\n",
    "\n",
    "# name of the <title> tag\n",
    "print(soup.title.name)\n",
    "\n",
    "# string value in the <title> tag\n",
    "print(soup.title.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how about if we want to find the first <div> tag?\n",
    "\n",
    "soup.div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the string value for the first <p> tag within the first <div>\n",
    "\n",
    "soup.div.p.string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the value of the \"class\" attribute of the first <div> under the first <div> (!?!)\n",
    "\n",
    "soup.div.div['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is how we'd find the the <a> tag\n",
    "\n",
    "soup.div.div.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### For You To Try\n",
    "\n",
    "# how would you find the url in the description?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `find` and `find_all` to search through the HTML to find certains tags and tag/attribute combinations. Let's take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all <p> tags\n",
    "for p in soup.find_all('p'):\n",
    "    print(p.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all <div class=\"author>...</div> tags\n",
    "for author_div in soup.find_all('div', attrs={'class': 'author'}):\n",
    "    print(author_div.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"The president is still lying...\"\n",
    "\n",
    "Coming back to the Trump's lies page, how can we use BeautifulSoup to parse our the lies and create a CSV data that we can use for our own analysis?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.nytimes.com/interactive/2017/06/23/opinion/trumps-lies.html'\n",
    "\n",
    "headers = {\n",
    "    'From': '<put your email here>'\n",
    "}\n",
    "\n",
    "# http request\n",
    "response = get(url, headers=headers)\n",
    "\n",
    "# run the HTML through BeautifulSoup\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# print out a pretty version of the BeautifulSoup object\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still a mess! Let's open up the [link](https://www.nytimes.com/interactive/2017/06/23/opinion/trumps-lies.html) in Chrome and view the HTML there. Remember the `View Page Source` option that allows us to peek under the covers of any web page? Another great resource is [Chrome Developer Tools](https://developer.chrome.com/devtools) which gives you an even greater look under the hood! \n",
    "\n",
    "Do we see any patterns in the NYTimes HTML?\n",
    "\n",
    "```html\n",
    "<span class=\"short-desc\"><strong>Jan. 21&nbsp;</strong>“I wasn't a fan of Iraq. I didn't want to go into Iraq.” <span class=\"short-truth\"><a href=\"https://www.buzzfeed.com/andrewkaczynski/in-2002-donald-trump-said-he-supported-invading-iraq-on-the\" target=\"_blank\">(He was for an invasion before he was against it.)</a></span></span>&nbsp;&nbsp;<span class=\"short-desc\"><strong>Jan. 21&nbsp;</strong>“A reporter for Time magazine — and I have been on their cover 14 or 15 times. I think we have the all-time record in the history of Time magazine.” <span class=\"short-truth\"><a href=\"http://nation.time.com/2013/11/06/10-things-you-didnt-know-about-time/\" target=\"_blank\">(Trump was on the cover 11 times and Nixon appeared 55 times.)</a></span></span>&nbsp;&nbsp;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can find each lie between the <span class=\"short-desc\"> and </span> tags\n",
    "\n",
    "for lie in soup.find_all('span', attrs={'class': 'short-desc'}):\n",
    "    print(lie.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how would we find each date?\n",
    "\n",
    "for lie in soup.find_all('span', attrs={'class': 'short-desc'}):\n",
    "    date = lie.find('strong')\n",
    "    print(date.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There's another way!**\n",
    "\n",
    "BeautifulSoup tags have a `contents` attribute returns a `list` of all of the tags children. The children in this case are all of the tags and strings nested under a tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the tag \"contents\"\n",
    "\n",
    "for lie in soup.find_all('span', attrs={'class': 'short-desc'}):\n",
    "    print(lie.contents)\n",
    "    print('---\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another way to get the date\n",
    "\n",
    "for lie in soup.find_all('span', attrs={'class': 'short-desc'}):\n",
    "    date = lie.contents[0].string\n",
    "    print(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how about getting the actual lie?\n",
    "\n",
    "for lie in soup.find_all('span', attrs={'class': 'short-desc'}):\n",
    "    lie_text = lie.contents[1].string\n",
    "    print(lie_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CSS Selectors**\n",
    "\n",
    "We can also use CSS selectors with BeautifulSoup. Pairing SelectorGadget with BeautifulSoup is easy with the `.select()` method. You can copy your CSS selector directly as an argument. So give it a try! Here we pull just the dates of the lies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.select(\".short-desc strong\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which we can now iterate over! Full circle!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting It All Together\n",
    "\n",
    "Now that we have the scraping part knocked out (congrats!), what if we wanted to save the data to a local `csv` file, or to pandas, to do further analysis? How might we do that?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from csv import writer\n",
    "\n",
    "output = writer(open(\"lie.csv\",\"w\"))\n",
    "output.writerow([\"date\",\"description\"])\n",
    "\n",
    "for lie in soup.find_all('span', attrs={'class': 'short-desc'}):\n",
    "    lie_text = lie.contents[1].string\n",
    "    lie_date = lie.contents[0].string\n",
    "    output.writerow([lie_date,lie_text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#x1f3c6; **Challenge round!** &#x1f3c6;\n",
    "\n",
    "Pick one or two of these tasks and use your skills with web scraping to answer the question. In each case, there is a URL and a data question attached to it. These come mainly from an excellent list compiled by Dan Nguyen at Stanford.\n",
    "\n",
    ">Site: [https://analytics.usa.gov/](https://analytics.usa.gov/)<br>\n",
    "Task: Number of people visiting US Government web sites now<br><br>\n",
    "Site: [http://www.state.gov/r/pa/ode/socialmedia/](http://www.state.gov/r/pa/ode/socialmedia/)<br>\n",
    "Task: The number of Pinterest accounts maintained by U.S. State Department embassies and missions<br><br>\n",
    "Site: [https://petitions.whitehouse.gov/](https://petitions.whitehouse.gov/)<br>\n",
    "Task: Number of petitions that have reached their goal<br><br>\n",
    "Site: [https://www.faa.gov/air_traffic/flight_info/aeronav/aero_data/](https://www.faa.gov/air_traffic/flight_info/aeronav/aero_data/)<br>\n",
    "Task: Number of airports with existing construction related activity<br><br>\n",
    "Site: [https://www.osha.gov/pls/imis/establishment.html](https://www.osha.gov/pls/imis/establishment.html)<br>\n",
    "Number of OSHA enforcement inspections involving Wal-Mart in California since 2014<br><br>\n",
    "Site: [https://www.tdcj.state.tx.us/death_row/dr_scheduled_executions.html](https://www.tdcj.state.tx.us/death_row/dr_scheduled_executions.html)<br>\n",
    "Task: Number of days until Texas's next scheduled execution <br><br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
