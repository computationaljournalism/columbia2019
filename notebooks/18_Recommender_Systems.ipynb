{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Family (and distance) ###\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "<img src=https://www.genealogy.math.ndsu.nodak.edu/images/genealogy_skeleton.png width=600>\n",
    "\n",
    "Yesterday, my PhD advisor, <a href=https://statistics.berkeley.edu/people/chuck-stone> Charles J. Stone</a>, passed away. It followed a long stay in hospice care. Chuck was a consumate theoretician, studying problems that are so relevant now as AI and deep learning swallow the planet. Today's lecture is moved up a little from our original plan -- in honor of Chuck. \n",
    "\n",
    "For those of you who report on science, you probably understand the strong ties between student and supervisor. It's a fraught relationship full of strange power dynamics. But the metaphor of family, of ancestry, is hard to escape. In my case, the math geneology project lets me trace my academic family tree. From Chuck...\n",
    "\n",
    "<img src=https://github.com/computationaljournalism/columbia2019/raw/master/images/cs1.jpg width=500>\n",
    "\n",
    "... to his adisor, Samuel Karlin at Stanford, and Chuck's academic brothers and sisters.\n",
    "\n",
    "<img src=https://github.com/computationaljournalism/columbia2019/raw/master/images/cs2.jpg width=500>\n",
    "\n",
    "Following back far enough, we get to my great great great great grandfather Gauss... like Gaussian distribution, the bell curve, you know. (Not too shabby.)\n",
    "\n",
    "<img src=https://github.com/computationaljournalism/columbia2019/raw/master/images/cs4.jpg width=500>\n",
    "\n",
    "The idea of family here is an interesting one. Academics inherit much from their advisors. Your advisor instills in you a particular theoretical or methodological stance in your field. Science is not nearly as fixed as it wants you to believe and, following Latour, every paper is more like a knife fight than a quiet stack of papers. \n",
    "\n",
    "Your advisor arms you and sends you into the world -- a crusade to advance his view of the world. An advisor will supervise many students, your brothers and sisters. Together, you take the training, the aesthetic sense into the world. It frames what you do, why you do it, how you talk about it... **and the kind of expert source you make.**\n",
    "\n",
    "Talk to  statistician at Duke and they will be Bayesian, talk to one at Berkeley and they will be a Frequentist. You think it's all math, but the two camps disagree on the nature of learning from data. That fued is softening, but remnants remain. This is true of every corner of science. Battlefields abound.\n",
    "\n",
    "**Chuck's work**\n",
    "\n",
    "We are moving a topic up to talk a little about Chuck's work. Here's one of his most cited papers. Citations are like the super powers you take into an academic squabble. They are your reputation, your stamp on the field. Your legacy. If a paper is never cited, even if it contains the cure to cancer, it has vanished.\n",
    "\n",
    "Here is [Chuck's first shocking paper.](https://www.jstor.org/stable/2240947?Search=yes&resultItemClick=true&searchText=charles&searchText=j.&searchText=stone&searchText=statistics&searchUri=%2Faction%2FdoAdvancedResults%3Fgroup%3Dnone%26amp%3Bf1%3Dall%26amp%3Bc2%3DAND%26amp%3Bc3%3DAND%26amp%3Bf4%3Dall%26amp%3Bc1%3DAND%26amp%3Bc4%3DAND%26amp%3Bc6%3DAND%26amp%3Bacc%3Don%26amp%3Bc5%3DAND%26amp%3Bf3%3Dall%26amp%3Bq1%3Dstatistics%26amp%3Bq0%3Dcharles%2Bj.%2Bstone%26amp%3Brefreqid%3Dsearch%253Ae5d562cce9bc86bb1d1d634dafcb256f%26amp%3Bf2%3Dall%26amp%3Bf5%3Dall%26amp%3Bsi%3D1%26amp%3Bso%3Dold%26amp%3Bf0%3Dau%26amp%3Bf6%3Dall&ab_segments=0%2Fdefault-2%2Fcontrol&refreqid=search%3A9593ed6740ce5dce77bcb3c367310798)\n",
    "\n",
    "<img src=https://github.com/computationaljournalism/columbia2019/raw/master/images/download.gif width=800>\n",
    "\n",
    "What is it about? Essentially, it says how well you can expect a machine learning algorithm to perform. We all know, or feel intuitively, that the more data you have in a survey, the better your answers. Chuck quantifies this improvement with data, the \"rate of convergence\" in a situation in which we are trying to learn the relationship between a group of predictors and some outcome variable. \n",
    "\n",
    "Characteristics of a parole case and the decision. Circumstances of an operation and a physician's propensity to injur a patient. So many prediction problems. So much data. How well can we do with more data? Better, sure, but are some procedures, some ways of learning, better than others? **What's the best we can expect to do?** That, that is the question! Tis nobler to prove, in one theorem, an optimal rate of improvement governing all estimators... than to suffer the slings and arrows of a patchwork set of decisions and the small proofs and weak ambitions of lesser mathematicians. \n",
    "\n",
    "<img src=https://secure.i.telegraph.co.uk/multimedia/archive/02890/hamlet_2890643b.jpg width=400>\n",
    "\n",
    "Chuck was a giant. He was concerned with high-dimensional problems -- spaces you can't envision, or we lack physical intuation as we live in 3 dimensions. Lots of predictors. How do you help people understand their relationship? Statisticians believe, or many do, that their models reflect, to some degree, the state of nature. This can be a problem as the statistician is a kind of translator -- from the data and math to states of nature. Chuck was concerned with explainability, but not at the cost of performance. He and co-authors (also giants) developed decision-tree technology to for modeling that strike this balance between legibility and prediction.\n",
    "\n",
    "<img src=https://images.gr-assets.com/books/1356139192l/1016135.jpg>\n",
    "\n",
    "These models are extremely powerful and we'll talk about them in another lecture. They have appeared in journalistic projects as handy models. Here's one from the NYTimes.\n",
    "\n",
    "<img src=https://static01.nyt.com/images/2008/04/16/us/0416-nat-subOBAMA.jpg>\n",
    "\n",
    "Midway through the presidential primaries, data on counties (their demographics, economics, previous voting record, etc.) paired with how they voted. With this, predict how the counties in states yet to hold a primary will vote and, as a bonus, what might go into that determination. The tree. Chuck proved how the accuracy of this representation improves with the number of counties you observed, say. Data volume tied to the quality of the prediction. That was Chuck.\n",
    "\n",
    "His final really big splash was associated with a series of named presentation called the Wald lectures. These he folded into a single paper that capped off a line of research pursued by his students, their students and other converts encountered along the way. \n",
    "\n",
    "<img src=https://github.com/computationaljournalism/columbia2019/raw/master/images/download-1.gif width=800>\n",
    "\n",
    "In this piece, he proposes an elaboration of tree models, one that fashions its predictions by a  combination of simple function evaluations, representing \"main effects\" and \"interactions\" and elaborating on the way regression modeling works. We'll get to regression later. \n",
    "\n",
    "The bottom line is that Chuck was concerned with legibility, with learning, with battling what has been referred to as \"the curse of dimensionality.\" The more data you have on a person or a row in a table, the higher the dimensionality of the problem and the harder it is to make predictions. Why? In some sense it's geometry. \n",
    "\n",
    "Most of the predictors Chuck works with depend on \"distances\" and make predictions from points that are nearby the conditions you'd like to make predictions about. But as you compute distances in higher and higher dimensions, the nearest point to you in a data set and the farthest point are essentially the same. The data spreads like distant planets orbiting your center point and it's hard to find close points to predict from. The statisticians white whale. \n",
    "\n",
    "So today we will spend time thinking about distances, about the structure of data. There are two simple PDF's [here](https://github.com/computationaljournalism/columbia2019/raw/master/docs/18_geometry1.pdf) and [here](https://github.com/computationaljournalism/columbia2019/raw/master/docs/18_geometry2.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=https://github.com/computationaljournalism/columbia2018/raw/master/images/maxresdefault.jpg width=500>\n",
    "    \n",
    "\n",
    "**Collaborative filtering**\n",
    "\n",
    "Now we are going to lay the groundwork for a cluster of computational tools that are riled under the names \"statistical modeling\", \"machine learning\", and even \"Artifical Intelligence\". We've seen various learning systems in previous lectures, and today we're going to examine so-called **recommender system**. In the material above, we talked about the basics of a **content-based** recommendation system. In the case of news, for example, recommended articles could be surfaced if you express interest in one topic or another. Other information about the stories could also be used to produce a ranking -- you might also include the number of \"likes\" or tweets, the publisher, the publication date and so on. \n",
    "\n",
    "This is the first of three approaches Alex Spangher wrote about in his NYT piece on their [next generation news recommender](https://open.blogs.nytimes.com/2015/08/11/building-the-next-new-york-times-recommendation-engine/). The second approach can be implemented without any information about content, and instead relies on how users rate or access content. On Amazon or Yelp or TripAdvisor, we have ratings that people provide for services. At the Times, we don't really have ratings, but instead we have whether or not someone read an article. **Collaborative filtering** uses patterns in these activities to make recommendations. (We \"filter\" items, recommending just a few, and this is a \"collaborative\" act because it relies on the input of others.)\n",
    "\n",
    "At the heart of collaborative filtering is something called a \"utility matrix\" or \"incidence matrix\". It's a table where rows refer to users (shoppers, readers), say, and columns are items (news articles, consumer goods). A recommendation system \"fills in\" or predicts how the user in row i will react to the object represented by column j -- that is, the table entry (i,j). Oh, math!\n",
    "\n",
    "So, here's a cartoon. Let's decide if we are going to recommend a great article on Climate Change to Marie. Marie is represented by row i in the incidence matrix and our Climate Change article is represented by column j. In row i we have 0's and 1's -- a 0 if Marie has not read the article and a 1 if she has. Column j is also filled with 0's and 1's, but this time an entry is 0 if the corresponding reader hasn't selected the article and 1 if he has. Got it? A row refers to the choices by a single user, and a column refers to the choices made by all our users for a single article. \n",
    "\n",
    "Now, we have two possible ways to use our incidence matrix, our table of 0's and 1's, to predict whether Marie will like our Climate Change article. The first is to find readers that are \"similar\" to Marie in terms of reading habits. That is, they have read some of the same articles or have 0's and 1's in the same places across their rows. We can take some number of readers who are most similar to Marie (the number always being called k) and then see what fraction read the article in question. In the image below, the gray band refers to the k readers who are similar to Marie and the red stripe highlights a column representing their reading of the single article. \n",
    "\n",
    "<img src=https://github.com/computationaljournalism/columbia2018/raw/master/images/abcabc.001.jpeg width=700>\n",
    "\n",
    "The second approach to fill in the (i,j) entry of a table like this would be to look at  articles similar to the Climate Change piece we want to recommend to Marie. Articles are similar if they were read (or not) by the same people. We can then take the k most similar articles and see what fraction of them Marie has read. If it's high, we recommend the article to her. The figure below is just the flip of the one we've seen. The grey band is now similar articles to the given one and the red stripe looks at whether or not Marie read these. \n",
    "\n",
    "<img src=https://github.com/computationaljournalism/columbia2018/raw/master/images/abcabc.002.jpeg width=700>\n",
    "\n",
    "This seems pretty straightforward (I hope). The techniques here are referred to ask \"k-nearest neighbors\" (or KNN or kNN). It's actually a pretty powerful machine learning (well, statistical) technique. And we leverage this kind of scheme all the time -- your doctor, for example, distills \"you\" into a row in a table, a height, a weight, a blood pressure, maybe some facts from your medical history. She then gives you advice about dropping a few pounds, say by what the medical profession knows about people like you. The idea is that the health outcomes of people \"similar\" to you can help predict what's around the corner for you. \n",
    "\n",
    "The key ideas here are pretty fundamental and **they have to do with distance.** Rows, for example, can be compared. In the case of collaborative filtering, we can evaluate one reader relative to another, marking some as close (likeminded, similar tastes) and others as far (red state, different tastes). This is a basic, mathematical abstraction that machine learning applies to all tables. Rows are points in a \"high-dimensional space\". (The same is true for columns, of course, except that it's typical to have tables -- spreadsheets-- organized so that rows refer to units of observation and columns refer to their characteristics.)\n",
    "\n",
    "Let's see an incidence matrix from a real application. This is a little perverse, but it was the best I could do, try as I might."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=https://github.com/computationaljournalism/columbia2018/raw/master/images/recommender-systems-13-638.jpg width=500>\n",
    "    \n",
    "\n",
    "**Collaborative baking**\n",
    "\n",
    "So far, this is pretty straightforward (I hope). The techniques here are referred to ask \"k-nearest neighbors\" (or KNN or kNN). It's actually a pretty powerful machine learning (well, statistical) technique. And we leverage this kind of scheme all the time -- your doctor, for example, distills \"you\" into a row in a table, a height, a weight, a blood pressure, maybe some facts from your medical history. She then gives you advice about dropping a few pounds, say by what the medical profession knows about people like you. The idea is that the health outcomes of people \"similar\" to you can help predict what's around the corner for you. \n",
    "\n",
    "The key ideas here are pretty fundamental and **they have to do with distance.** Rows, for example, can be compared. In the case of collaborative filtering, we can evaluate one recipe relative to another, marking some as close (similar ingredients) and others as far (different tastes). This is a basic, mathematical abstraction that machine learning applies to all tables. Rows are points in a \"high-dimensional space\". (The same is true for columns, of course, except that it's typical to have tables -- spreadsheets-- organized so that rows refer to units of observation and columns refer to their characteristics.)\n",
    "\n",
    "Let's see an incidence matrix from our cakes data set. It is weenie, but I'm down the rabbit hole now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "cakes = read_csv(\"https://github.com/computationaljournalism/columbia2019/raw/master/data/cakes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cakes.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first five rows of the `incidence` data frame. Again, the matrix holds a 0 in row i and column j if the recipe in row i is missing ingredient j. It's 1 otherwise. Remember how many cakes we had..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cakes.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, 1,000 recipes and 381 ingredients. As we commented in the last notebook, row and column sums can be important. Summing down the columns counts how many recipes (out of 1,000) contain each ingredient. Here we `apply()` the `sum()` function down columns, indicated with `axis=0`. We sort the resuts and see that many ingredients appear just once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cakes.apply(sum,axis=0).sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we compute just how many ingredients appear in one recipe. We take the column sum and ask for a boolean outcome, `True` if there was one recipe and `False` for more. Adding these up turns `True` to 1 and `False` to zero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum( cakes.apply(sum,axis=0) == 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That means 126 or about 13% are used by just one recipe. How many have two? Three? Less than five?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How should we judge similarity between two recipes? Again, we want them to have use much the same ingredients. A simple measure for that is the **Jaccard metric.** It is good for rows (or columns) that are made up of 0's and 1's. Essentially it looks at how many 1's the two recipes have in common, divided by the total number of ingredients they require. Well, you subtract that ratio from 1. So, if two recipes have nothing in common, the metric will be 1, and if they have everything in common, it will be 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jaccard distance\n",
    "\n",
    "def dist(a,b):\n",
    "    \n",
    "    intersection = sum((a+b)==2)\n",
    "    union = sum((a+b)>=1)\n",
    "    \n",
    "    # print intersection,union\n",
    "    \n",
    "    return 1.0-(0.0+intersection)/union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try out the function. We'll make two series (Pandas' representation of a row or column) of 0's and 1's and computes their Jaccard distance. See if this makes sense. Change the 0's and 1's to all agree or disagree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import Series\n",
    "x = Series([1,0,1,1])\n",
    "y = Series([0,0,1,1])\n",
    "dist(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, to compute the distance between two recipes, we need to access rows. This is done using `.iloc[]` subsetting. We haven't seen this yet, but it gives us fine-grained control over the rows or columns we want to extract. The notation is \n",
    ">`row selection, column selection`\n",
    "\n",
    "Before the comma refers to rows and after to columns. We can use single integers for a single row or column, a slice like `5:10` to get a range,  and the empty slice `:` to ask for all the rows or columns. The result is a Pandas Series. \n",
    "\n",
    "Here is recipe with ID 40. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cakes.iloc[40,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This recipe has read 10 ingredients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(cakes.iloc[40,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[c for c in cakes.columns if cakes[c][10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is the distance between recipe 40 and recipe 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[c for c in cakes.columns if cakes[c][40]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two recipes share 8 ingredients and between them, there are 12 total ingredients. That means the distance is 1-8/12 = 1-2/3 = 1/3. Let's see!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist(cakes.iloc[40,:],cakes.iloc[10,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes! \n",
    "\n",
    "Now, this loop iterates through the entire set of rows and calculates the distance between recipe 40 and the rest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = Series([ dist(cakes.iloc[40,:],cakes.iloc[i,:]) for i in range(1000)])\n",
    "distances.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we sort the distaances and look at the distance of the 25th nearest recipe to number 40."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "close = distances.sort_values().reset_index(drop=True)[25]\n",
    "close"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then subset just the nearby cakes, those with distance less than 0.417. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "near_cakes = cakes[(distances<=close) & (distances>0)]\n",
    "near_cakes.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pull out the ingredients in recipe 40, storing them in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingredients = [ing for ing in cakes.columns if cakes[ing][40]]\n",
    "ingredients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we sort the ingredients according to the number of recipes that contain them. We then leave out all the ingredients that are already in cake 40. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations = near_cakes.apply(sum,axis=0).sort_values(ascending=False).reset_index()\n",
    "recommendations[~recommendations[\"index\"].isin(ingredients)].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's wrap these two into a function and have a look at a few cakes and what we recommend to add."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist(a,b):\n",
    "    \n",
    "    intersection = sum((a+b)==2)\n",
    "    union = sum((a+b)>=1)\n",
    "    \n",
    "    # print intersection,union\n",
    "    \n",
    "    return 1.0-(0.0+intersection)/union\n",
    "\n",
    "def recommend(c,k=10,recipes=cakes):\n",
    "    \n",
    "    n_recipes = recipes.shape[0]\n",
    "    \n",
    "    ingredients = [ing for ing in cakes.columns if cakes[ing][c]]\n",
    "    print(\"Ingredients in recipe\",c)\n",
    "    print(ingredients)\n",
    "    \n",
    "    distances = Series([dist(recipes.iloc[c,:],recipes.iloc[i,:]) for i in range(n_recipes)])\n",
    "    close = list(distances.sort_values())[k]\n",
    "\n",
    "    near_recipes = recipes[(distances<=close) & (distances>0)]\n",
    "    recommendations = near_recipes.apply(sum,axis=0).sort_values(ascending=False).reset_index()\n",
    "    \n",
    "    return recommendations[~recommendations[\"index\"].isin(ingredients)][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommend(920,25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommend(230,25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommend(430,25)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
