{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unix, Mongo and a Computer in the Cloud ☁️\n",
    "----------------------------------------\n",
    "\n",
    "All of the data gathering and computation we've done up until now has been done on our own laptops. Going forward, there will be many times when we'll need to collect data 24x7. For this, we'll need a computer that runs 24x7 (i.e. not our laptop). We will also want to store some of our data in a database, which gives us a lot more power and flexibility over storing data in, say, CSV files. Before we get to setting up a computer in the cloud and using our first database, we'll talk a bit about the Unix operating system and the power of Unix commands (what we'll use to drive our cloud computer). This notebook may span a few classes, but here's a quick overview of what we'll cover:\n",
    "\n",
    "1. Unix Introduction\n",
    "2. Set up a computer in the cloud using Amazon Web Services (AWS)\n",
    "3. Unix commands (how we drive our cloud computer)\n",
    "3. twarc - a comand-line based Twitter API\n",
    "4. Mongo database - where we can store our data in the cloud\n",
    "5. How to talk to our Mongo database from the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unix \n",
    "---------------------------\n",
    "\n",
    "Today we step back to a simpler time when we interacted with computers through a handful of typed commands. Specifically, we'll deal in some pretty old magic -- UNIX commands. At one level, you can think of these as tools to help you manipulate programmatically the basic **stuff of a computer**. We'll work with files and folders and running jobs. These tools come from a time when computers looked like this.\n",
    "\n",
    "<img align=center src=http://history-computer.com/ModernComputer/Software/images/Dennis-Ritchie-Ken-Thompson-and-PDP11-UNIX-1972.jpg>\n",
    "\n",
    "Pictured above are two of the developers of UNIX, Dennis Ritchie and Ken Thompson. To give their work some context, let's define terms. The start of this notebook is a little chatty, but it will also be one of the most practial we'll have all term.\n",
    "\n",
    "      __\n",
    "    o-''|\\_____/)\n",
    "     \\_/|_)     )\n",
    "        \\  __  /\n",
    "        (_/ (_/    \n",
    "\n",
    "**An operating system** \n",
    "\n",
    "An operating system is a piece of software (code) that organizes and controls hardware and other software so your computer behaves in a flexible but predictable way.\n",
    "\n",
    "Most devices that contain a computer of some kind will have an OS. Operating systems appear when the appliance will have to deal with new applications, complex user input and possibly changing requirements of its function. In addition to a laptop or desktop computer, your DVR, smartphone and even your automobile all have operating systems.\n",
    "\n",
    "The computer you're using to run this notebook probably has a Windows, MacOS or Linux operating system. \n",
    "\n",
    "**Your computer**\n",
    "\n",
    "Let's think about your computer a little more deeply -- it consists of several components. **The Central Processing Unit** (CPU) or microprocessor (a microchip) is a complete computational engine, capable of carrying out a number of basic commands (perform simple arithmetic calculations, store and retrieve information from its memory, and so on). \n",
    "\n",
    "The CPU itself has the capacity to store some amount of information, and when it needs more space, it moves data to another kind of chip known as **Random Access Memory** (RAM) — \"random access\" as opposed to, say, sequential access to memory locations. \n",
    "\n",
    "Your computer also has one or more storage devices that can be used to organize and store data — hard disks or drives store data magnetically, while solid state drives again use special chips. (A solid state drive is a larger, more sophisticated version of your traditional thumb drive.)\n",
    "\n",
    "**Operating systems, again**\n",
    "\n",
    "Your operating system, then, manages all of your computer’s resources, providing layers of abstraction for both you, the user, as well as developers who are writing new programs for your computer.\n",
    "\n",
    "With the emergence of so-called **cloud computing**, we imagine a variety of computing resources \"out there\" on the web that we can execute -- think about the variety of APIs we've encountered that do various smart things for us or to our data. In this model, computations are performed elsewhere, and your own computer might function more as a \"browser\" receiving results -- Google’s Chrome operating system is \"minimalist\" in this sense. \n",
    "\n",
    "But let’s not get ahead of ourselves. The computers you are probably sitting at are running the Mac OS which is built on a Unix platform. Let’s spend some time talking about Unix.\n",
    "\n",
    "\n",
    "                /)-_-(\\        /)-_-(\\\n",
    "                 (o o)          (o o)\n",
    "         .-----__/\\o/            \\o/\\__-----.\n",
    "        /  __      /              \\      __  \\\n",
    "    \\__/\\ /  \\_\\ |/                \\| /_/  \\ /\\__/\n",
    "         \\\\     ||                  ||      \\\\\n",
    "         //     ||                  ||      //\n",
    "         |\\     |\\                  /|     /|\n",
    "         \n",
    "**UNIX history**\n",
    "\n",
    "In 1964, Bell Labs (the research arm of AT&T) partnered with MIT and GE to create Multics (for Multiplexed Information and Computing Service) -- here is the vision they had for computing\n",
    "\n",
    ">“Such systems must run continuously and reliably 7 days a week, 24 hours a day in a\n",
    "way similar to telephone or power systems, and must be capable of meeting wide\n",
    "service demands: from multiple man-machine interaction to the sequential processing\n",
    "of absentee-user jobs; from the use of the system with dedicated languages and\n",
    "subsystems to the programming of the system itself”\n",
    "\n",
    "Bell Labs pulled out of the Multics project in 1969, a group of researchers at Bell Labs started work on Unics (Uniplexed information and computing system) because initially it could only support one user; as the system matured, it was renamed UNIX, which isn’t an acronym for\n",
    "anything. Ritchie simply says that UNIX is a \"somewhat treacherous pun on Multics.\"\n",
    "\n",
    "While this seems like quite a long time ago, consider how Dennis Ritchie described UNIX support for programming.\n",
    "\n",
    ">Ritchie observes: “What we wanted to preserve was not just a\n",
    "good environment in which to do programming, but a system\n",
    "around which a fellowship could form. We knew from\n",
    "experience that the essence of communal computing, as\n",
    "supplied by remote-access, time-shared machines, is not just\n",
    "to type programs into a terminal instead of a keypunch, but to\n",
    "encourage close communication.” The theme of computers\n",
    "being viewed not merely as logical devices by as the nuclei of\n",
    "communities was in the air; 1969 was also the year the\n",
    "ARPANET (the direct ancestor of today’s Internet) was\n",
    "invented. The theme of “fellowship” would resonate all through\n",
    "UNIX’s subsequent history.\n",
    "<br><br>From [\"The Art of Unix Programming\"](http://www.catb.org/esr/writings/taoup/) by Raymond\n",
    "\n",
    "In Multics, we find the first notion of a hierarchical file system -- software for\n",
    "organizing and storing computer files and the data they contain in UNIX, files are\n",
    "arranged in a tree structure that allows separate users to have control of their own\n",
    "areas. Think a system of folders or directories -- one folder can contain files and other folders and so on. A tree! UNIX began (more or less) as a file system and then an interactive shell emerged to let you examine its contents and perform basic operations. And these are what we will focus on today.\n",
    "\n",
    "**The UNIX kernel and shell**\n",
    "\n",
    "The **UNIX kernel** is the part of the operating system that provides other programs\n",
    "access to the system’s resources (the computer’s CPU or central processing unit, its\n",
    "memory and various I/O or input/output devices).\n",
    "\n",
    "The **UNIX shell** is a command-line interface to the kernel — keep in mind that UNIX\n",
    "was designed by computer scientists for computer scientists and the interface is not\n",
    "optimized for novices. (The term \"shell\" is general in that a shell is the outermost\n",
    "interface to the inner workings of the system it surrounds -- where have we seen this idea before?)\n",
    "\n",
    "The UNIX shell is a type of program called an interpreter — in this case, think of it as a\n",
    "text-based interface to the kernel. It operates in a simple loop: It accepts a command, interprets it, executes the command and waits for another. Very obedient. The shell displays a prompt to tell you that it is ready to accept a command. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a Mac, you can open the Terminal application and be greeted with a happy UNIX prompt. On a Windows laptop, you can run a Unix shell but you'll need something like [WSL](https://docs.microsoft.com/en-us/windows/wsl/install-win10), [cygwin](https://www.cygwin.com/) or a virtual machine.\n",
    "\n",
    "              /\\___/\\\n",
    "              `)9 9('\n",
    "              {_:Y:.}_\n",
    "    ----------( )U-'( )----------\n",
    "\n",
    "Since not everyone in class is on a Mac, we are going to set up our first cloud computer (using Amazon) where we can practice our Unix commands together. For those on a Mac, you can run Unix commands directly from the notebook using \"cell magic\" syntax.\n",
    "\n",
    "One last comment. There are several versions of a UNIX shell. Why might we want different kinds of interfaces to our computer? Well, it turns out that some shells are good for interactive work (allowing you to hit the Tab key and have a command \"autocomplete\") while others have additional programming support to help you make \"scripts\" (think of the move from single commands to functions in Python). The **sh**, or [the Bourne Shell](https://en.wikipedia.org/wiki/Bourne_shell), is an old standby, whereas **bash**, or [the Bourne Again Shell](https://www.gnu.org/software/bash/), combines many characteristics of different shells together (bashing them together).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our Own Computer in the (Amazon) Cloud\n",
    "\n",
    "Alright, we are now going to venture into the cloud. Our first task is to create a computer \"out there\". Something that isn't our laptop. It won't cost us anything as we will rent a **computer equivalent of a 70s sedan**. But the process is the same if we were renting a Porsche. We will use [Amazon's EC2 (Elastic Compute Cloud) service.](https://aws.amazon.com/ec2/) \n",
    "\n",
    "```\n",
    "                .--~~,__\n",
    "   :-....,-------`~~'._.'\n",
    "    `-,,,  ,_      ;'~U'\n",
    "     _,-' ,'`-__; '--.\n",
    "    (_/'~~      ''''(;\n",
    "```\n",
    "\n",
    "If you don't already have an Amazon Web Services account, please [set that up](https://portal.aws.amazon.com/billing/signup) now (it's free!).\n",
    "\n",
    "Once you have an account, head over to the [EC2 site](https://aws.amazon.com/ec2/). In the upper righthand corner, click on the yellow \"Sign In to the Console\" button and use your Amazon account. \n",
    "\n",
    "We will want to sign in to the console which should land us on a page that looks like this.<br><br>\n",
    "\n",
    "<img src=https://github.com/computationaljournalism/columbia2018/raw/master/images/screen.jpg style=\"width: 65%; border: #000000 1px outset;\"/>\n",
    "<br>\n",
    "\n",
    "Select the \"EC2\" services which, in turn, will take us to a screen that should look a lot like this.<br><br>\n",
    "\n",
    "<img src=https://github.com/computationaljournalism/columbia2018/raw/master/images/screen2.jpg \n",
    "style=\"width: 65%; border: #000000 1px outset;\"/>\n",
    "<br>\n",
    "\n",
    "From here, we can \"launch an instance\", or, rather, startup a computer for our personal use. This computer will remain awake and operational until we decide to take it down. Click on the blue \"Launch Instance\" button in the middle of the page. \n",
    "\n",
    "1. Use the blue \"Select\" button to choose the first kind of computer you are offered on the page, **\"Amazon Linux AMI 2018.03.0 (HVM), SSD Volume Type\"**. Don't sweat all the lingo, just know that this is a Linux computer, and the operating system was configured by Amazon. \n",
    "2. After you select the type of computer, you will be asked for the \"size\" specifications. You will select from the \"General purpose\" family, a **t2.micro** computer. This baby is free! OK you have 1Gb of RAM and about 8Gb  of storage, and not all of your jobs will work with this free choice. *But*, for now, it gets the point across. With the \"t2.micro\" selected, scroll to the bottom of the page and click the blue **\"Review and Launch\"** button.\n",
    "3. Scroll to the bottom of the page and select **\"Launch\"**. You will be immediately prompted to create so-called key pair. From the menu, choose to create a new key pair and give it a name. This will cause a file (ending in .pem) to be downloaded to your computer. We'll do more with that in a second. For now, hit **\"Launch Instance\"** and away we go!\n",
    "4. Finally, on the landing page, click on the link that looks like **The following instance launches have been initiated: i-097....743ca** to trot over and have a look at your new arrival.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our \"70s Sedan\"\n",
    "\n",
    "This is the equivalent of the computer we just started:\n",
    "\n",
    "<img src=\"http://chrisoncars.com/wp-content/uploads/2010/06/Ford_Fairmont_sedan_2.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's Connect to Our Computer in the Cloud\n",
    "\n",
    "The following instructions on how to connect to our cloud computer are for those on a Mac (or Linux). For those using Windows, you can use a program called Putty (instructions are [here](http://www.dorusomcutean.com/ssh-ec2-instance-windows-using-putty/)).\n",
    "\n",
    "First, **open a new Terminal window.** This will be our  portal to the computer we just created. \n",
    "\n",
    "Back on the AWS EC2 site, click the button next to your new \"instance\" in the table. At the top of the EC2 console, you'll see a grey button asking you to **\"Connect\"**. It will pop up a small window that tells you what to do with your key file. The command `chmod` is a UNIX command you type into the terminal window that changes the \"permissions\" on the key file. I usually make a folder called Credentials and put the file in there. The 400 says that only you can look at this file and that the other users of your laptop (guests, say) can't see it. \n",
    "\n",
    "If you are on a Mac: in your terminal let's type a few Unix commands to set the proper permissions on the credentials file:\n",
    "<br><br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`cd Downloads`\n",
    "<br><br>\n",
    "`cd` stands for \"change directory\" and takes us to whatever directory we specify. Here, make sure you use the name of the directory where you downloaded the .pem file.\n",
    "<br><br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`chmod 400 (yourkey.pem)`\n",
    "<br><br>\n",
    "Security! Then, use the `ssh` command they provide, again in the terminal window, with the right path to your key file. It should look something like:\n",
    "<br><br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`ssh -i (yourkey.pem) ec2-user@(your machine)`\n",
    "\n",
    "\n",
    "**Copy this into your Terminal window.** The command `ssh` stands for secure shell and is your window to the new computer. To get there you have to provide your key (which is why you want to keep it safe) and the address of the machine. You should be greeted with something like this...<br><br>\n",
    "\n",
    "```\n",
    "\n",
    "       __|  __|_  )\n",
    "       _|  (     /   Amazon Linux AMI\n",
    "      ___|\\___|___|\n",
    "\n",
    "https://aws.amazon.com/amazon-linux-ami/2017.09-release-notes/\n",
    "6 package(s) needed for security, out of 8 available\n",
    "Run \"sudo yum update\" to apply all updates.\n",
    "[ec2-user@ip-172-30-0-108 ~]$\n",
    "```\n",
    "<br><br>\n",
    "With the dollar sign being your very own UNIX prompt out in the cloud! Ha! Now, let's learn some Unix commands so we know how to drive this 70s sedan....\n",
    "\n",
    "```\n",
    "         __\n",
    "        /  \\\n",
    "       / ..|\\\n",
    "      (_\\  |_)\n",
    "      /  \\@'\n",
    "     /     \\\n",
    " _  /  `   |\n",
    "\\\\/  \\  | _\\\n",
    " \\   /_ || \\\\_\n",
    "  \\____)|_) \\_)\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start off, let's download a file and give you a sense of what UNIX commands are capable of.\n",
    "\n",
    "## NOTE: the following Unix commands are meant to be run in a terminal window, not the notebook!\n",
    "\n",
    "We will work on [this file](https://github.com/computationaljournalism/columbia2019/raw/master/data/columbia.txt), which is a log file from the columbia.edu web server. To download the file from github to our EC2 instance, we can use a command-line tool called [wget](https://www.gnu.org/software/wget/). Run the following command on our EC2 instance we just created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wget https://github.com/computationaljournalism/columbia2019/raw/master/data/columbia.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see some output that looks like this:\n",
    "\n",
    "```2019-03-13 14:30:03 (47.0 MB/s) - ‘columbia.txt’ saved [1048576/1048576]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've downloaded a file from our course web server. It's called columbia.txt and it's now on our cloud computer. We are going to examine it using some simple UNIX commands. \n",
    "\n",
    "A few for exploring your folders: **pwd, ls, cd**<br><br>Making and removing folders (directories): **mkdir, rmdir**<br><br>Copying, renaming and removing files: **cp, mv, rm**<br><br>\n",
    "\n",
    "\n",
    "         |\\_/|                  \n",
    "         | @ @   Woof! \n",
    "         |   <>              _  \n",
    "         |  _/\\------____ ((| |))\n",
    "         |               `--' |   \n",
    "     ____|_       ___|   |___.' \n",
    "    /_/_____/____/_______|\n",
    "    \n",
    "<br><br>First, **pwd** or \"print working directory\" will tell you which folder you're in. For the notebook, this means the folder your data and notebook file are being stored in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The command **ls** lists the contents of a folder. Compare this list below to what you  see when you use your finder to examine the same folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unix commands can be modified by adding one or more **options**. In the case of\n",
    "ls, we can add a \"-l\" for the \"long\" form of the output and a \"-a\" for all directories that\n",
    "begin with a “.” Another useful option is \"-h\" for humanly readable output or \"-G\" for\n",
    "color (which will only show up in the Terminal and not the notebook).\n",
    "\n",
    "The long printout below tells us the size of each file and what we can do to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ls -l "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand what this all means, let's look at the first line (for me).\n",
    "\n",
    "<pre>\n",
    "-rw-rw-r--   1 ec2-user ec2-user 1048576 Mar 13 14:33 columbia.txt\n",
    "</pre>\n",
    "\n",
    "Let's split things up a bit. \n",
    "\n",
    "<pre>\n",
    " f1  f2   f3   f4  f5  f6        f7        f8       f9            f10\n",
    "  -  rw-  rw-  r--  1  ec2-user  ec2-user  1048576  Mar 13 14:33  columbia.txt\n",
    "</pre>\n",
    "\n",
    "Let's walk through the different fields.\n",
    "\n",
    "**f1**: - for File, d for Directory, l for \"Link\"\n",
    "\n",
    "**f2**, **f3** and **f4**: These are \"permissions\" that mean you can read (r), write (w) and execute (x) a file, or not (-). They come in three different clusters specifing permissions\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**f2**: The owner has over the file,<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**f3**: The group has over the file, and<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**f4**: Everybody else has over the file\n",
    "\n",
    "**f5**: This field specifies the number of links or directories inside this directory.\n",
    "\n",
    "**f6**: This is the user who owns the file or directory.\n",
    "\n",
    "**f7**: The group that file belongs to, and any user in that group will have the permissions given in the third field over that file.\n",
    "\n",
    "**f8**: The size in bytes, you may modify this by using the -h option (humanly readable) together with -l this will have the output in k,M,G so it's just a bit easier to understand.\n",
    "\n",
    "**f9**: The date of last modification\n",
    "\n",
    "**f10**: The name of the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aside: Why a command line?**\n",
    "\n",
    "While interacting with a computer by typing in commands might seem primitive, it has its advantages (there are reasons why it's hanging around).\n",
    "\n",
    "**Agile** — It is designed tok be very interactive, supporting exploratory\n",
    "analysis; it is also close to the “filesystem” which means the tools are\n",
    "close to the data you’re working with\n",
    "\n",
    "**Scalable** — You are interacting with your computer by typing\n",
    "commands and not through a graphical user interface (GUI) which\n",
    "means your instructions can be combined into a file or script and\n",
    "reused\n",
    "\n",
    "**Extensible** — New tools are being developed for the command line\n",
    "on a daily basis, being written in a variety of languages but all usable\n",
    "in the same way as the original tools that appeared in the 1960’s and\n",
    "1970s\n",
    "\n",
    "**Ubiquitous** — It is hard to find a computer system that you’ll\n",
    "purchase (desktop or laptop) and if you soar into “the cloud” you will\n",
    "likely encounter the various computers you find there through a\n",
    "command line interface\n",
    "\n",
    "       _=,_\n",
    "    o_/6 /#\\\n",
    "    \\__ |##/\n",
    "     ='|--\\\n",
    "       /   #'-.\n",
    "       \\#|_   _'-. /\n",
    "        |/ \\_( # |\" \n",
    "       C/ ,--___/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Back to the drill**\n",
    "\n",
    "The command **man** will provide you with help on any Unix command. You simply\n",
    "supply the name of the command you are interested in as an argument. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "man ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the **man** command above, the string \"ls\" is passed as an **argument** that tells UNIX which \"data\" to work with. Here's another example of an argument. \n",
    "\n",
    "The command **head** does what you might expect given our exposure to Pandas. It prints out the first 10 lines of a file, the name of which you pass as an argument. Here we look at the first 10 lines of \"columbia.txt\". How do you get the last 10?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head columbia.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "man head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A UNIX command  will often involve both arguments and options. Here we tell **head** to only print out the first three lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head -3 columbia.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And where there's a head, you'll also find a tail!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tail columbia.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Web access logs**\n",
    "\n",
    "OK what kind of data do we have? This is the so-called [combined log format](https://httpd.apache.org/docs/1.3/logs.html) from an Apache web server. Whenever you browse a web site (in this case, [www.journalism.columbia.edu](http://www.journalism.columbia.edu)), there is a program responding to your requests. Want the home page? Want information about the Dual Degree? You request the HTML page and that request is recorded as a single line in the log file. Then, to render the page, your browser might need some CSS files or JavaScript files or just some simple images. The subsequent requests for these objects are also recorded, one line each, in the log file. \n",
    "\n",
    "So the log file is growing with each user's visit. Requests are logged to the bottom of the file in time, so the oldest entries are at the top of the file and the newest at the bottom. If many people are looking at the site at the same time, their requests are interleaved in the file, as it records requests in time order. \n",
    "\n",
    "Each line in the log file hold these values\n",
    "\n",
    ">IP address<br>\n",
    "Identity<br>\n",
    "Userid<br>\n",
    "date<br>\n",
    "Request<br>\n",
    "Status<br>\n",
    "Bytes<br>\n",
    "Referrer<br>\n",
    "Agent\n",
    "\n",
    "Let's compare this information with the first line (oldest request) in our file. (Notice that these log lines are really long and so \"wrap around\" the cell and can look like two or more lines.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head -1 columbia.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the visit was from someone using the address 128.59.40.117 at 17/Apr/2016:06:27:25. The request was for a file called \"robots.txt\" which describes where automated programs are allowed to scrape on the site. [The 200 means](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes) that the transmission was completed and that 469 bytes were sent. The user agent is not a browser but a \"crawler\" which means an automated scraper that is sucking up our content, presumably because it's feeding a search engine.\n",
    "\n",
    "Finally, a UNIX command can help us figure out the IP address. The `whois` command does not come pre-installed on our Amazon t2.micro but we can use it on our Mac, try an [online service](https://www.ultratools.com/tools/ipWhoisLookup) to do it for us, or even install the program on our instance (by typing `sudo yum install jwhois`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whois 128.59.40.117"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The newest entries in the file are obtained from the bottom of the file. The last few lines are displayed with **tail**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tail -1 columbia.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This last request has a timestamp of \"17/Apr/2016:09:12:53\". That means we have captured about 3 hours worth of activity on our site. How many requests is that? The command **wc** tells us how many lines, words and characters are in a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc columbia.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in our three hours we have 4,000 or so requests. What other questions might we ask of the data? We might want to know how many different IP addresses appear in the data set. Or maybe how many different status codes. \n",
    "\n",
    "We can use the command **cut** to select specific items from the file. Here we pass options that include \"-d\" (a character to be used as a delimiter defining separate fields in the file) and \"-f\" (to specify which fields to cut from the file). \n",
    "\n",
    "Below we define individual fields as being separated by a blank space character and then ask for just the first field, the IP address."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cut -d\" \" -f1 columbia.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut -d\" \" -f10 columbia.txt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at one of the log lines above and make sure you understand that the 10th field (as defined by spaces) is the number of bytes transferred. \n",
    "\n",
    "Below, use another delimiter to pull out the month the request was made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The options for the fields to keep include lists separated by commas and ranges defined by a hyphen. The next two are fields 1 and 10 and then fields 1 through 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cut -d\" \" -f1,10 columbia.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cut -d\" \" -f1-3 columbia.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we're getting tired of seeing 4000 lines of output scroll by. We can catch the output and \"pipe\" it into the command that restricts us to 10 lines, **head**. The vertical bar \"|\" is known as a pipe and it takes the output of one command (cut, below) and pipes it as input to the next command (head, below). \n",
    "\n",
    "The net result is printing just 10 lines of fields 1 and 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cut -d\" \" -f1,10 columbia.txt | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As its name suggests, the command **sort** will order the rows in our file. By default it\n",
    "uses alphabetical order but the option \"-n\" lets you sort numerically instead. Below we **cut** out just the IP's and then \"redirect the output\" to a file called \"ips.txt\". We then sort the IP addresses and put the sorted result in a filed called \"ips_sorted.txt\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cut -d\" \" -f1 columbia.txt > ips.txt\n",
    "sort ips.txt > ips_sorted.txt\n",
    "head -100 ips_sorted.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With UNIX pipes, we can avoid the extra files and just get the sorted data directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cut -d\" \" -f1 columbia.txt | sort | head -100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(You will end the display with a red box saying that the **head** command only allowed 100 lines to be printed and not all of the output from **sort**. It's OK.)\n",
    "\n",
    "Next, the command **uniq** will remove repeated adjacent lines in a file, so if your file is sorted, it will return just the unique rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uniq ips_sorted.txt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or in one line..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cut -d\" \" -f1 columbia.txt | sort | uniq | head -100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The command **uniq** has an option \"-c\" that returns the counts of each row in the file. If we apply it to \"ips_sorted.txt\", we'll get two columns -- one is how many requests were made in our 3 hour window by the IP address and the second is the IP address."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uniq -c ips_sorted.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, preferably, in one line..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cut -d\" \" -f1 columbia.txt | sort | uniq -c | head -100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can add a second sort to this pipeline to sort in reverse numerical order (using options -r and -n) the **uniq**'d file, giving us the most frequently seen IPs first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cut -d\" \" -f1 columbia.txt | sort | uniq -c | sort -rn | head -25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So 207.46.13.69 was seen 139 times. What is this address?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "whois 207.46.13.69"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's owned by Microsoft. We can use a filtering command known as **egrep** to pull just the lines that match a regular expression pattern (in quotes). So we might do the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "egrep \"207\\.46\\.13\\.69\" columbia.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we could see what the pattern is using [regexper.com](https://regexper.com/#207%5C.46%5C.13%5C.69).\n",
    "\n",
    "We could save these lines in a new file if we wanted to do more work. But for now, we see that they are all running \"bingbot\" which is the spider (scraper) for the Bing search engine. Let's see how many times \"bingbot\" is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "egrep \"bingbot\" columbia.txt | wc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So 481 out of our 4000 or so requests were from Bing. \n",
    "\n",
    "The referrer field is number 11. It records the link someone clicked on to get to the page they're requesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cut -d\" \" -f11 columbia.txt | head -100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here we look at just referrers that are Google."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cut -d\" \" -f11 columbia.txt | egrep \"google\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can clean up a little. Let's remove our two files of IP addresses. First, find them..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ls -l ips*.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then use **rm** to remove them, with a follow up **ls** to make sure they're gone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rm ips.txt\n",
    "rm ips_sorted.txt\n",
    "ls -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your turn**\n",
    "\n",
    "Come up with three questions about the visitors to the site and answer them using simple UNIX commands. Recall we've seen \n",
    "\n",
    ">**pwd, ls, rm, mv, cp<br><br> head, tail, wc, <br><br> cut, sort, uniq,<br><br> grep**\n",
    "\n",
    "This is a pretty powerful pipeline!\n",
    "<br><br>\n",
    "\n",
    "    _     /)---(\\          /~~~\\\n",
    "    \\\\   (/ . . \\)        /  .. \\\n",
    "     \\\\__)-\\(*)/         (_,\\  |_)\n",
    "      \\_       (_         /   \\@/    /^^^\\\n",
    "      (___/-(____) _     /      \\   / . . \\\n",
    "                   \\\\   /  `    |   V\\ Y /V\n",
    "                    \\\\/  \\   | _\\    / - \\\n",
    "                     \\   /__'|| \\\\_  |    \\\n",
    "                      \\_____)|_).\\_).||(__V\n",
    "                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Twitter in the Cloud**\n",
    "\n",
    "To monitor Twitter remotely, we can use an application called twarc. It is a command line tool for archiving tweets. It handles all the rate limits and lets you worry about what you're going to do with the data once it's puddledup. First, using your terminal window that is logged into the EC2 t2.micro computer, install twarc. It's a Python application so...\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`sudo pip install twarc`\n",
    "\n",
    "Ah, `sudo`. That's a command that basically promotes you to administrator while you install twarc. Think about your laptop. When you install software, you have to type in a password because you need to have super powers to put files on certain parts of the computer. Your guests, for example, probably don't have this ability. \n",
    "\n",
    "Once you have installed tward, you should configure it with your keys. Have them ready from Twitter ([go to apps.twitter.com](https://apps.twitter.com/)) and type\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`twarc configure`\n",
    "\n",
    "OK that done, we can now start monitoring Twitter! Here's a simple \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`twarc timeline realDonaldTrump`\n",
    "\n",
    "OK that had a lot of stuff streaming by. Essentially, you received all of realDonaldTrump's tweets, up to the rate limit. We didn't ask to do anything with them so they just printed out. twarc has a lot of great features that let you do things like follow people and watch their tweets in real time. According to [https://tweeterid.com/](https://tweeterid.com/), realDonaldTrump as a twitter id of 25073877. Here's how we follow this account.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`twarc filter --follow  25073877`\n",
    "\n",
    "And now the printout is slower, but it is meant to be printing both realDonaldTrump's tweets and retweets. You end this parade by entering Cntl-C to kill the twarc job.\n",
    "\n",
    "twarc is quite useful. Here is [detailed documentation](https://github.com/DocNow/twarc). I'd advise using it where you can.\n",
    "\n",
    "**Storage: Moving files back and forth into the cloud**\n",
    "\n",
    "Now, rather than have the data stream by, we could capture it in a file. Recall our \"redirect\" that dumps output into a file. Here we store realDonaldTrump's timeline in a file called `trump.json`. \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`twarc timeline realDonaldTrump > trump.json`\n",
    "\n",
    "To make use of it, let's copy it from our cloud computer back to our desktop. So type \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`exit`\n",
    "\n",
    "and you should end up with a prompt that looks more like your laptop where you started. Now, hit the \"up arrow\" key on your keyboard while you are in the terminal window. This will recall your last command. You can then alter it to the following (keeping `yourkey` and `yourmachine` as they are in your terminal window. \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`scp -i (yourkey.pem) ec2-user@(your machine):trump.json .`\n",
    "\n",
    "This command is \"secure copy\" -- it uses your credentials or key to move the file `trump.json` from your amazon computer to your laptop. If you want to copy some other file `abc.txt` to the cloud machine you would do this.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`scp -i (yourkey.pem) abc.txt ec2-user@(your machine).com:`\n",
    "\n",
    "So the syntax is \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`scp -i (yourkey.pem) from_file to_file`\n",
    "\n",
    "... make sure you see this by comparing the two lines above. Now, we can read that file in (maybe you have to put `trump.json` into the folder where your notebook is located)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from json import loads\n",
    "\n",
    "# read in the tweets as strings from the file - one line per tweet\n",
    "tweetstrings = open('trump.json').readlines()\n",
    "\n",
    "# for the first 10 strings, load them into python objects (dictionaries)\n",
    "# and print out the text of the tweet\n",
    "\n",
    "for t in tweetstrings[:10]:\n",
    "    tweet = loads(t)\n",
    "    print(tweet[\"full_text\"])\n",
    "    print(\"-------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "            ,/A\\,\n",
    "          .//`_`\\\\,\n",
    "        ,//`____-`\\\\,\n",
    "      ,//`[_ROVER_]`\\\\,\n",
    "    ,//`=  ==  __-  _`\\\\,\n",
    "   //|__=  __- == _  __|\\\\\n",
    "   ` |  __ .-----.  _  | `\n",
    "     | - _/       \\-   |\n",
    "     |__  | .-\"-. | __=|\n",
    "     |  _=|/)   (\\|    |\n",
    "     |-__ (/ a a \\) -__|\n",
    "jgs  |___ /`\\_Y_/`\\____|\n",
    "          \\)8===8(/\n",
    "```\n",
    "\n",
    "**Storage: Installing a Mongo database**\n",
    "\n",
    "That's cool but we can do way way better. Let's go back to your computer in the Amazon cloud and install a database. We will use something called MongoDB (Mongo from hu*mongo*us.). You can [read about the project here.](https://www.mongodb.com/). It is an example of a new-ish breed of data bases that have emerged. They are called NoSQL (for non-SQL or \"not only\" SQL) and signal a break from the relational model (which, weirdly, we will come back to). According to the Mongo site, some examples of this new breed include\n",
    "\n",
    "* **Document databases** pair each key with a complex data structure known as a document. Documents can contain many different key-value pairs, or key-array pairs, or even nested documents.\n",
    "* **Graph stores** are used to store information about networks of data, such as social connections. Graph stores include Neo4J and Giraph.\n",
    "* **Key-value stores** are the simplest NoSQL databases. Every single item in the database is stored as an attribute name (or 'key'), together with its value. Examples of key-value stores are Riak and Berkeley DB. Some key-value stores, such as Redis, allow each value to have a type, such as 'integer', which adds functionality.\n",
    "* **Wide-column stores** such as Cassandra and HBase are optimized for queries over large datasets, and store columns of data together, instead of rows.\n",
    "\n",
    "Mongo is a document database, where the documents are represented by JSON strings. This kind of flexibility is perfect for our Twitter data, as a tweet is just a JSON object. To install Mongo on our cloud machine, `ssh` over there and let's do the following. Oh we have [taken these instructions mainly from here.](https://github.com/SIB-Colombia/dataportal-explorer/wiki/How-to-install-node-and-mongodb-on-Amazon-EC2)\n",
    "\n",
    "1. Secure shell over to your Amazon machine using the `ssh` command from above.\n",
    "2. The version of `pip` for Linux (or a version) is something called `yum`. It stands for \"Yellow Dog Updater, Modified\". It was the package manager for a version of Linux called Yellow Dog Linux (which was an early UNIX OS that ran on a Mac!). Our first command will be to update `yum` itself. First, making sure all its packages are current.\n",
    "<br><br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`sudo yum check-update`<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`sudo yum update`\n",
    "<br><br> Then, `yum` needs a database of projects to look through and our next commands will be to update that list. Don't worry too much about this, it's just adding a \"repo\" to the places where `yum` looks  for code to install.  \n",
    "<br>\n",
    "`echo \"[mongodb-org-4.0]\n",
    "name=MongoDB Repository\n",
    "baseurl=https://repo.mongodb.org/yum/amazon/2013.03/mongodb-org/4.0/x86_64/\n",
    "gpgcheck=1\n",
    "enabled=1\n",
    "gpgkey=https://www.mongodb.org/static/pgp/server-4.0.asc\" |\n",
    "sudo tee -a /etc/yum.repos.d/mongodb-org-4.0.repo`\n",
    "<br><br>\n",
    "3. Next, install MongoDB. It's just like using `pip` except that we have to `sudo` for administrator powers and then use `yum` for a Linux app. \n",
    "<br><br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`sudo yum install -y mongodb-org`\n",
    "<br><br>\n",
    "4. We are using the /var/lib/mongo folder to save our database data, a log file and so on. These defaults are fine.\n",
    "<br><br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`sudo mkdir /var/lib/mongo/data`\n",
    "<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`sudo mkdir /var/lib/mongo/log`\n",
    "<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`sudo mkdir /var/lib/mongo/journal`\n",
    "<br><br>\n",
    "5. Set the storage items (data, log, journal) to be owned by the user (mongod) and group (mongod) that MongoDB will be starting under:\n",
    "<br><br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`sudo chown mongod:mongod /var/lib/mongo/data`\n",
    "<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`sudo chown mongod:mongod /var/lib/mongo/log`\n",
    "<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`sudo chown mongod:mongod /var/lib/mongo/journal`\n",
    "<br><br>\n",
    "6. Set the MongoDB service to start at \"boot\" (if you ever have to reboot your machine) and activate Mongo!\n",
    "<br><br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`sudo chkconfig mongod on`\n",
    "<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`sudo /etc/init.d/mongod start`\n",
    "<br><br>\n",
    "7. Have a look around! MongoDB has a shell (everything does!) that you can use to look at data, etc. There's not much to do now except maybe create a new database and a user who can read and write into the database.\n",
    "<br><br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`mongo`\n",
    "<br><br>\n",
    "Not much to do just yet. I mean you can ask for help, and maybe `show databases`. So let's add some data. But first, <br><br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`quit()` \n",
    "<br><br>\n",
    "out of here. \n",
    "\n",
    "```\n",
    " __/ / \\\n",
    "|    |/\\\n",
    "|_--\\   \\              /-\n",
    "     \\   \\-___________/ /\n",
    "      \\                :\n",
    "      |                :\n",
    "      |       ___ \\    )\n",
    "       \\|  __/     \\  )\n",
    "        | /         \\  \\\n",
    "        |l           ( l\n",
    "        |l            ll\n",
    "        |l            |l\n",
    "       / l           / l\n",
    "       --/           --\n",
    "```\n",
    "\n",
    "**Loading data into Mongo**\n",
    "\n",
    "Now, let's store data. We'll take realDonaldTrump's timeline and dump it into a database. The instructions [are given here](https://gist.github.com/edsu/ac57715ac0a2fec3bc64).\n",
    "<br><br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`twarc timeline realDonaldTrump | mongoimport --db tweets --collection realDonaldTrump` \n",
    "<br><br>\n",
    "This command uses twarc, asks for realDonaldTrump's timeline and then pipes the output (pipes!) into  a command `mongoimport` to bring the data into our database. The database is called `tweets` and the particular collection of documents is called `realDonaldTrump`. \n",
    "\n",
    "Think of this structure as having one database per project and then multiple collections of documents (JSON data) in each database.\n",
    "<br><br>\n",
    "To see what we've done, let's get into Mongo \n",
    "<br><br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`mongo`\n",
    "<br><br>\n",
    "and then look at the databases\n",
    "<br><br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`show databases`\n",
    "<br><br>\n",
    "to see your `tweets` and then \n",
    "<br><br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`use tweets`\n",
    "<br><br>\n",
    "to switch to that database. We can then see what collections are available in this database.\n",
    "<br><br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`show collections`\n",
    "<br><br>\n",
    "And we can see what's there. Maybe we count the tweets we've recorded in each collection.\n",
    "<br><br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`db.realDonaldTrump.count()`\n",
    "<br>\n",
    "\n",
    "The structure of this command in the Mongo shell is `db.collectionname.action`. We can do things like find all the tweets from `@realDonaldTrump` that were retweeted over 80,000 times:\n",
    "<br><br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`db.realDonaldTrump.find({retweet_count:{$gt:80000}})`\n",
    "\n",
    "This gives us the entire tweet object for each tweet that was RT'd over 80,000 times. We can modify our `find` command to have it return only the retweet_count, and the text of the tweet (as opposed to the whole thing). Here is the code:\n",
    "<br><br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`db.realDonaldTrump.find({retweet_count:{$gt:80000}},{retweet_count:1,full_text:1})`\n",
    "\n",
    "In general, the `find()` command searches for JSON documents and uses a dictionary syntax to find them. Here we searched for the `retweet_count` field, looking for those well retweeted tweets. \\$gt and \\$lt are ways to specify ranges. The second argument of `find()` gives a dictionary that tells you what data to keep. The value 1 means keep.\n",
    "\n",
    "The Mongo shell is really powerful. The Mongo site [has great documentation on `find()` and other commands](https://docs.mongodb.com/manual/reference/method/db.collection.find/). Now, we are often going to access a database from the comfort of some other computing environment. In this case, Python. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Before we move on, let's set up a few users in our database to allow us to administer it (later) and access it remotely. First, we'll create our admin user. Please change the password here to something you can use later. Copy the following lines in to mongo:\n",
    "<pre>\n",
    "use admin\n",
    "db.createUser(\n",
    "  {\n",
    "    user: \"admin\",\n",
    "    pwd: \"SomeStrongPasswordHere4JustYou\",\n",
    "    roles: [ { role: \"userAdminAnyDatabase\", db: \"admin\" }, \"readWriteAnyDatabase\" ]\n",
    "  }\n",
    ")\n",
    "</pre>\n",
    "\n",
    "2. To prepare our database for remote access, we'll set up a user (with a password). In mongo you can copy the following:\n",
    "<pre>\n",
    "use tweets\n",
    "db.createUser({\n",
    "  user: 'journalist',\n",
    "  pwd: 'secret',\n",
    "  roles: [{ role: 'readWrite', db:'tweets'}]\n",
    "})\n",
    "</pre>\n",
    "and then get out\n",
    "<br><br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;quit()\n",
    "<br><br>\n",
    "\n",
    "3. Next, we want to open up Mongo to talk to the outside world. This means changing its configuration file. Here we comment out one command and remove the comment from another. We are using an old old UNIX command called Sed for the Stream Editor.\n",
    "<br><br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`sed -i 's/bindIp.*/bindIp: 0.0.0.0/' /etc/mongod.conf`\n",
    "<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`sudo sed -i 's/^#security/security/' /etc/mongod.conf `\n",
    "<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`sudo sed -i \"/^security/a  \\ \\ \\ authorization: 'enabled'\" /etc/mongod.conf`\n",
    "<br><br>\n",
    "4. Return to your EC2 Console and click on the instance in the upper pane of the console. Below you will see details about your computer and scroll down to \"Security Groups\". It should probably be \"launch-wizard-1\". Click on it and look at its security rules. Click on the \"Inbound\" tab. You see port 22 on the machine is open for `ssh` communication (that includes the secure shell and secure copy). Click \"Edit\" and then \"Add Rule\". You will want to select \"Custom TCP Rule\" (the default) and then Port 27017 and the access IP of 0.0.0.0/0, meaning every computer can connect. If there was just one IP address that needed your data, you could put it there instead. Hit \"Save\" and go back to your Terminal logged into the Amazon computer.\n",
    "5. On the Amazon computer restart Mongo with its new user and new network aware self.\n",
    "<br><br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`sudo service mongod restart`\n",
    "<br><br>\n",
    "And there we are. Mongo is up and running and we can now talk to it. Let's! To do this in Python, we need to install PyMongo. Yay!\n",
    "\n",
    "```\n",
    "            __\n",
    "(\\,--------'()'--o\n",
    " (_    ___    /~\"\n",
    "  (_)_)  (_)_)\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Python and Mongo - PyMongo, of course!**\n",
    "\n",
    "PyMongo lets us access a database from the comfort of our notebook. There are a few tutorials online, but [the basic documentation is here.](http://api.mongodb.com/python/current/tutorial.html) For the most part, the structure and commands are similar to those in Mongo itself. The documents stored in a Mongo database can be nested structures and, as we have seen, we often do a little work to get them into regular table format. \n",
    "\n",
    "Before we go too far, let's install PyMongo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "pip install pymongo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to import the MongoClient function. It takes a specification for the location of a Mongo database and returns a client object. Working with the client object  is a bit like typing into the `mongo` shell as we did above. Here we create the client and then access the \"tweets\" database.\n",
    "\n",
    "**NOTE** you will need to put the IP address of your Amazon EC2 instance in the code below where we connect to our Mongo database. Look back at the EC2 console web page and find the `IPv4 Public IP` for our instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "client = MongoClient(\"mongodb://journalist:secret@put_your_instance_IP_here:27017/tweets\")\n",
    "type(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db = client.tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expression above matches the  Mongo shell expression. We can also use something a bit more Python inspired  and ask for the \"tweets\" database using subset notation. The expression above is equivalent to the the one  below. both return a link to a database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db = client[\"tweets\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, we can ask for the different `collection_names()` that we have loaded. Remember that a document database consists of different collections of documents. In our case a document is a tweet and our collections refer to @realDonaldTrump tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.collection_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recalling our simple Mongo commands, here  are the number  of documents (tweets) in each collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db[\"realDonaldTrump\"].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here we create a variable `trump` that represents the `realDonaldTrump` collection of tweets. This keeps us from continually typing out the full expression. From here we can look at one  tweet..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump = db[\"realDonaldTrump\"]\n",
    "trump.find_one()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... or maybe iterate through several that match a search criterion. That's one of the reasons to have a database in the first place -- we can make searching very fast. In the expression below, we form a search for all the tweets with where the language is \"undefined\". A search is literally expressed as another document. Here's our query.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`{\"lang\": \"und\"}`\n",
    "\n",
    "Let's see how many tweets match this criterion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump.find({\"lang\": \"und\"}).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use a regular expression to find a pattern in the source and not just a literal match. Here we find how many times he tweeted from his iPhone. For this, we make use of an operator to specify the documents of interest.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`{\"source\": {\"$regex\":\"iphone\"}}`\n",
    "\n",
    "Let's see how many there are..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump.find({\"source\": {\"$regex\":\"(iphone)\"}}).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the expression below, we form a search for all the tweets with a `retweet_count` larger than 20000. There are special operators $gt and \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`{\"retweet_count\":{\"$gt\":20000}}`\n",
    "\n",
    "And count..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump.find({\"retweet_count\":{\"$gt\":20000}}).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "    ___\n",
    " __/_  `.  .-\"\"\"-.\n",
    " \\_,` | \\-'  /   )`-')\n",
    "  \"\") `\"`    \\  ((`\"`\n",
    " ___Y  ,    .'7 /|\n",
    "(_,___/...-` (_/_/ sk\n",
    "```\n",
    "\n",
    "Rather than just counting, we can iterate through the set to display our results. Here we search for tweets with a retweet count over 20,000 and we only keep the fields `text`, `retweet_count` and `user.screen_name` (the \".\" is how we index into an embedded document, `screen_name` being a key to the `user` dictionary of the tweet). The notation in the second dictionary assigns a value of `True` to a key if you want to keep the variable with that name and it assigns a `False` otherwise. (You will also see 1 and 0 instead -- remember `True` reduces to 1 and `False` to 0.) This is called **a projection**. The first two arguments to `find()` are `filter=` and `projection=`. \n",
    "\n",
    "Here we are leaving out Mongo's `_id` variable as it's an internal Mongo index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tweet in trump.find({\"retweet_count\":{\"$gt\":20000}},{\"full_text\":True,\"retweet_count\":True,\"user.screen_name\":True,\"_id\":False}):\n",
    "    print(tweet)\n",
    "    print(\"-------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The database can do quite a bit of work for you before you request  any data. You can, for example, look at all the tweets from realDonaldTrump, ordered by retweet count, but maybe with the largest retweet count first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import ASCENDING, DESCENDING\n",
    "\n",
    "for tweet in trump.find().sort(\"retweet_count\",DESCENDING).limit(10):\n",
    "    print(tweet[\"retweet_count\"],tweet[\"full_text\"])\n",
    "    print(\"-------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can filter these out using a regular expression (this one using a negative lookahead for RT) and then sorting the results. The point is that all this work is done in the database and we blisfully pull over just the data we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tweet in trump.find({\"full_text\":{\"$regex\":\"^(?!RT)\"}}).sort(\"retweet_count\",DESCENDING).limit(10):\n",
    "    print(tweet[\"retweet_count\"],tweet[\"full_text\"])\n",
    "    print(\"-------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, this lets us form subsets of data and creates a \"cursor\" that lets us walk through the data, processing things as we like. We can also take the data directly into a, you guessed it, Pandas data frame. We'll dive in to more of this in the coming lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary**\n",
    "\n",
    "We now have a number of options for working with data. We can store data  locally as JSON or CSV files, but these are relatively \"inert\". They have to be read into Python or some other system to make convenient searches, for example. Through a database, you have consistent storage that many people can access and you can use the computational engine  to filter, group and compute on data before you bring it into Python, say. So while your  data may be humongous, your interest might be in individual people. There is no need to hold gigabytes of data in memory when you only need a small plart.\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
